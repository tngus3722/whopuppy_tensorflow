{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "whopuppy_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b05SB04dx09i"
      },
      "source": [
        "# STEP 1  get image from google drive\r\n",
        "# STEP 2. resize and covert rgb\r\n",
        "# STEP 3. Set one-hot encoding\r\n",
        "# STEP 4. Save as numpy array\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "import os, glob, numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "caltech_dir = \"./drive/My Drive/images/New/clear_image/\"\r\n",
        "categories = [\"cocaspaniel\" , \"Husky\", \"Maltese\", \"Pome\", \"Retriever\",\"WelshiCorgi\",\"bichon\",\"chihuaua\",\"Bulldog\",\"Poodle\",\"Dalmatian\",\"Dachshund\",\"ShibaInu\",\"Beagle\",\"Pug\",\"Yorkshireterrier\"]\r\n",
        "#categories = [\"cocaspaniel\" , \"Husky\", \"Maltese\", \"Pome\", \"Retriever\",\"WelshiCorgi\",\"bichon\"]\r\n",
        "nb_classes = len(categories)\r\n",
        "\r\n",
        "image_w = 160\r\n",
        "image_h = 160\r\n",
        "\r\n",
        "pixels = image_h * image_w * 3\r\n",
        "\r\n",
        "X = []\r\n",
        "y = []\r\n",
        "\r\n",
        "for idx, cat in enumerate(categories):\r\n",
        "    \r\n",
        "    #one-hot 돌리기.\r\n",
        "    label = [0 for i in range(nb_classes)]\r\n",
        "    label[idx] = 1\r\n",
        "\r\n",
        "    image_dir = caltech_dir + \"/\" + cat\r\n",
        "    print(image_dir)\r\n",
        "    files = glob.glob(image_dir+\"/*.jpg\")\r\n",
        "    print(cat, \" 파일 길이 : \", len(files))\r\n",
        "    for i, f in enumerate(files):\r\n",
        "        #print(i, f)\r\n",
        "        img = Image.open(f)\r\n",
        "        img = img.convert(\"RGB\")\r\n",
        "        img = img.resize((image_w, image_h))\r\n",
        "        data = np.asarray(img)\r\n",
        "\r\n",
        "        X.append(data)\r\n",
        "        y.append(label)\r\n",
        "\r\n",
        "\r\n",
        "X = np.array(X)\r\n",
        "y = np.array(y)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\r\n",
        "\r\n",
        "np.save(\"./drive/My Drive/numpy_data/multi_imag_X_train.npy\", X_train)\r\n",
        "np.save(\"./drive/My Drive/numpy_data/multi_image_X_test.npy\", X_test)\r\n",
        "np.save(\"./drive/My Drive/numpy_data/multi_image_y_train.npy\", y_train)\r\n",
        "np.save(\"./drive/My Drive/numpy_data/multi_image_y_test.npy\", y_test)\r\n",
        "\r\n",
        "print(\"ok\", len(y))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpYBhDnYx182"
      },
      "source": [
        "#STEP1 GET numpy array from google drive\r\n",
        "#STEP2 data /255 \r\n",
        "#STEP3 Featcher batch\r\n",
        "#STEP4 Get base model MobileNetV2\r\n",
        "#STEP5 Get global featcher average\r\n",
        "#STEP6 Prediction layer add ( len categories ) \r\n",
        "#STEP7 Train dataset to model\r\n",
        "\r\n",
        "import os, glob, numpy as np\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\r\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\r\n",
        "\r\n",
        "config =  tf.compat.v1.ConfigProto()\r\n",
        "config.gpu_options.allow_growth = True\r\n",
        "session = tf.compat.v1.Session(config=config)\r\n",
        "\r\n",
        "train_examples = np.load('./drive/My Drive/numpy_data/multi_imag_X_train.npy', allow_pickle=True)\r\n",
        "test_examples = np.load('./drive/My Drive/numpy_data/multi_image_X_test.npy', allow_pickle=True)\r\n",
        "train_labels = np.load('./drive/My Drive/numpy_data/multi_image_y_train.npy', allow_pickle=True)\r\n",
        "test_labels = np.load('./drive/My Drive/numpy_data/multi_image_y_test.npy', allow_pickle=True)\r\n",
        "\r\n",
        "train_examples = train_examples.astype(float) / 255\r\n",
        "test_examples = test_examples.astype(float) / 255\r\n",
        "#X_train, X_test, y_train, y_test = np.load('./drive/My Drive/numpy_data/multi_image.npy', allow_pickle=True)\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\r\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "SHUFFLE_BUFFER_SIZE = 1000\r\n",
        "\r\n",
        "train_batches = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\r\n",
        "test_batches  = test_dataset.batch(BATCH_SIZE)\r\n",
        "\r\n",
        "for image_batch, label_batch in train_batches.take(1):\r\n",
        "   pass\r\n",
        "\r\n",
        "image_batch.shape\r\n",
        "\r\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n",
        "                                               include_top=False,\r\n",
        "                                               weights='imagenet')\r\n",
        "\r\n",
        "\r\n",
        "feature_batch = base_model(image_batch)\r\n",
        "print(feature_batch.shape)\r\n",
        "\r\n",
        "base_model.trainable = False\r\n",
        "\r\n",
        "#base_model.summary()\r\n",
        "\r\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\n",
        "feature_batch_average = global_average_layer(feature_batch)\r\n",
        "print(feature_batch_average.shape)\r\n",
        "\r\n",
        "prediction_layer = tf.keras.layers.Dense(len(categories),activation='softmax')\r\n",
        "prediction_batch = prediction_layer(feature_batch_average)\r\n",
        "print(prediction_batch.shape)\r\n",
        "\r\n",
        "\r\n",
        "model = tf.keras.Sequential([\r\n",
        "  base_model,\r\n",
        "  global_average_layer,\r\n",
        "  prediction_layer\r\n",
        "])\r\n",
        "\r\n",
        "base_learning_rate = 0.0001\r\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "model.summary()\r\n",
        "len(model.trainable_variables)\r\n",
        "\r\n",
        "initial_epochs = 10\r\n",
        "validation_steps=20\r\n",
        "model_dir = './drive/My Drive/model'\r\n",
        "with tf.device('/device:GPU:0'):    \r\n",
        "  if not os.path.exists(model_dir):\r\n",
        "    os.mkdir(model_dir)\r\n",
        "  model_path = model_dir + '/multi_img_classification.model'\r\n",
        "  checkpoint = ModelCheckpoint(filepath=model_path , monitor='val_loss', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', mode='auto', patience=7)\r\n",
        "  history = model.fit(train_batches,\r\n",
        "                    epochs=initial_epochs,\r\n",
        "                    validation_data=test_batches,\r\n",
        "                    callbacks=[checkpoint, early_stopping]\r\n",
        "                    )\r\n",
        "  \r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "ISLd9jtFypJ3",
        "outputId": "a4f06cbc-a778-4113-b10d-c2a5896e3b36"
      },
      "source": [
        "# draw train accuracy, validation accuracy\r\n",
        "# draw train loss, validation loss \r\n",
        "\r\n",
        "acc = history.history['accuracy']\r\n",
        "val_acc = history.history['val_accuracy']\r\n",
        "\r\n",
        "loss = history.history['loss']\r\n",
        "val_loss = history.history['val_loss']\r\n",
        "\r\n",
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(2, 1, 1)\r\n",
        "plt.plot(acc, label='Training Accuracy')\r\n",
        "plt.plot(val_acc, label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.ylim([min(plt.ylim()),1])\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(2, 1, 2)\r\n",
        "plt.plot(loss, label='Training Loss')\r\n",
        "plt.plot(val_loss, label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.ylabel('Cross Entropy')\r\n",
        "plt.ylim([0,1.0])\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-00a020cecfed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok-L7_-2y1mm"
      },
      "source": [
        "# go to fine tuning\r\n",
        "# down learning rate\r\n",
        "# learning start ex-train epoc to fine_tune_epochs\r\n",
        "# callback method save model if upgrade\r\n",
        "\r\n",
        "base_model.trainable = True\r\n",
        "\r\n",
        "fine_tune_at = 100\r\n",
        "\r\n",
        "#fine_tune_at` 층 이전의 모든 층을 고정\r\n",
        "for layer in base_model.layers[:fine_tune_at]:\r\n",
        "  layer.trainable =  False\r\n",
        "\r\n",
        "# 학습률 낮춤\r\n",
        "base_learning_rate = base_learning_rate/10\r\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "fine_tune_epochs = 10\r\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\r\n",
        "\r\n",
        "history_fine = model.fit(train_batches,\r\n",
        "                         epochs=total_epochs,\r\n",
        "                         initial_epoch =  history.epoch[-1],\r\n",
        "                         validation_data=test_batches,\r\n",
        "                         callbacks=[checkpoint, early_stopping]\r\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwesXmQTzHQf"
      },
      "source": [
        "\r\n",
        "# get model from google drive\r\n",
        "\r\n",
        "from keras.models import load_model\r\n",
        "model = load_model('./drive/My Drive/model/multi_img_classification.model') # 학습된 모델 \r\n",
        "#model = load_model('./multi_img_classification.model') # 학습된 모델 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiO5toBDzZCv"
      },
      "source": [
        "from urllib import request\r\n",
        "import time\r\n",
        "from io import BytesIO\r\n",
        "from PIL import Image\r\n",
        "import os, glob, numpy as np\r\n",
        "\r\n",
        "categories = [\"cocaspaniel\" , \"Husky\", \"Maltese\", \"Pome\", \"Retriever\",\"WelshiCorgi\",\"bichon\",\"chihuaua\",\"Bulldog\",\"Poodle\",\"Dalmatian\",\"Dachshund\",\"ShibaInu\",\"Beagle\",\"Pug\",\"Yorkshireterrier\"]\r\n",
        "image_w = 160\r\n",
        "image_h = 160\r\n",
        "\r\n",
        "url = input()\r\n",
        "\r\n",
        "# request.urlopen()\r\n",
        "res = request.urlopen(url).read()\r\n",
        "\r\n",
        "# Image open\r\n",
        "img = Image.open(BytesIO(res))\r\n",
        "img = img.convert(\"RGB\")\r\n",
        "img = img.resize((image_w, image_h))\r\n",
        "data = np.asarray(img)\r\n",
        "array = []\r\n",
        "array.append(data)\r\n",
        "array = np.array(array)\r\n",
        "array = array.astype(float) / 255\r\n",
        "\r\n",
        "prediction = model.predict(array)\r\n",
        "\r\n",
        "answer = []\r\n",
        "length = len(prediction[0])\r\n",
        "\r\n",
        "for i in range(length):\r\n",
        "    tmp = prediction[0][i]\r\n",
        "    answer.append([tmp*100,categories[i]])\r\n",
        "\r\n",
        "answer = sorted(answer, reverse =True, key = lambda x:x[0])\r\n",
        "\r\n",
        "result = []\r\n",
        "count= 0\r\n",
        "for i in answer:\r\n",
        "  result.append(i)\r\n",
        "  count+=1\r\n",
        "  if ( count == 5):\r\n",
        "    break\r\n",
        "\r\n",
        "print(result)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}